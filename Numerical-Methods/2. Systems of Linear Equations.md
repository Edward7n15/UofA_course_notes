# Linear Systems
#Linear: effects are proportional to their causes

#Linear_Transformation: $Lu=f$
- L is the proportionality
- u is the cause
- f is the effect

Even when relationships are nonlinear, they can often be usefully approximated locally
- derivative provides a local linear approximation, a tangent line, to a nonlinear curve

A system of linear equations $Ax=b$ 
- ==can the vector b be expressed as a linear combination of the columns of matrix A?==
- does b lie in span(A)
	- if yes, the system is #Consistent

#Square_Systems: the matrix has the same number of rows and columns

## Existence and Uniqueness
#Nonsingular:
- A has inverse
	- $AA^{-1}=I=A^{-1}A$
- $det(A)\not=0$
- $rank(A)=n$
- $\forall z\not=0,\,Az\not=0$

Otherwise, A is #Singular 

#Nonsingular matrices have unique solution because the inverse exists
- $x=A^{-1}b$
while #Singular matrices have infinite or no solutions
- $A(x+\gamma z)=b$ where $Az=0$ and $\gamma$ is any scalar
	- there is a vector ==annihilates== A so you can add any number of the result Az and not changing the RHS

We could summarized as follows:
- unique solution
	- A #Nonsingular 
	- b arbitrary
- infinite solutions
	- A #Singular 
	- $b\in span(A)$
		- #Consistent 
- no solution
	- A #Singular 
	- $b\not\in span(A)$
		- #Inconsistent

In 2D, each linear equation determines a straight line in the plane. The solution is the intersection point of two lines.
- #Singular is the case that the two lines are parallel therefore no solution or infinite solutions

# Sensitivity and Conditioning
## Vector Norms
![[Pasted image 20240227212904.png]]
- 1-norm
	- Manhattan norm
	- absolute sum
- 2-norm
	- Euclidean norm
	- distance between two points
- inf-norm
	- equals the maximum element
	- other elements will be ignored in infinite power

![[Pasted image 20240227221224.png]]
A norm with smaller p is an upper bound for the the next norm
![[Pasted image 20240227221338.png]]

properties
- a norm is positive if x is not zero
	- because of absolute sum
- scalar multiplier inside the norm can be extracted with an absolute sign
- addition inside a norm is the lower bound for the norm addition
	- triangle inequality
![[Pasted image 20240227221742.png]]
## Matrix Norms

A way to measure the size or magnitude of matrices.
- using underlying vector norm
![[Pasted image 20240227222409.png]]
#Induced or #Subordinate to the given vector norm x

The norm of a matrix measures the maximum stretching the matrix does to any vector, as measured in the given vector norm.

- 1-norm
	- maximum absolute column sum
	- the maximum column
- inf-norm
	- maximum absolute row sum
	- the maximum basic vector

Properties:
![[Pasted image 20240227222911.png]]
	Same as vector norms. But introducing multiplication bounds.

## Matrix Condition Number
$cond(A)=||A||\cdot||A^{-1}||$
- the more close to 1, the more well-conditioned
- measures ==singularity==
![[Pasted image 20240227223440.png]]
The ratio of the maximum relative stretching to the maximum relative shrinking that matrix does to any nonzero vectors.
- measures the amount of distortion of the unit sphere under transformation by the matrix
![[Pasted image 20240227223836.png]]
See the stretch to the unit circle. 

![[Pasted image 20240227224348.png]]

The determinant of a matrix is a YES-OR-NO indicator of singularity, and #Condition_Number is a good indicator of near singularity.

It is easy to compute inf-norm of a matrix but the inverse is challenging.
An approach is like this:
Assuming the inverse exists, then we have a unique solution
- $Az=y\rightarrow z=A^{-1}y$
Applying the norm's property
- $||z||=||A^{-1}y||\le||A^{-1}||\cdot||y||$
Therefore, we have the lower bound of the inverse
- $\frac{||z||}{||y||}\le||A^{-1}||$
The equal sign holds for some optimally chosen vector y
- expensive
- but a heuristic way is $A^Ty=c$ where c is a vector with $\pm 1$

## Error Bounds
![[Pasted image 20240227225556.png]]
With an error input, we have the true output and the deviation

Notice that the transformation of the input deviation is the output deviation because the transformation of true input is the true output which can be cancelled out in the equation above.
- $A\Delta x=\Delta b$
- $\Delta x=A^{-1}\Delta b$
- $||\Delta x||=||A^{-1}\Delta b||\le||A^{-1}||\cdot||\Delta b||$

We also know that
- $Ax=b$
- $||Ax||=||b||$
- $||b||=||Ax||\le||A||\cdot||x||$
- $\frac1{||x||}\le\frac{||A||}{||b||}$

Combine these two
- $\frac{||\Delta x||}{||x||}\le||A||\cdot||A^{-1}||\cdot\frac{||\Delta b||}{||b||}=cond(A)\cdot\frac{||\Delta b||}{||b||}$
- the condition number an amplification factor that bounds the maximum relative change in the solution x due to a given relative change in the RHS vector b

That was a forward way, let's try the backward way:
![[Pasted image 20240227231057.png]]
![[Pasted image 20240227231120.png]]

Another way to understand this is 

![[Pasted image 20240228003750.png]]
A way to visualize the condition number
![[Pasted image 20240228004506.png]]

Remember that the foregoing analysis using norms provides a bound on the relative error in the ==largest components== of the solution vector.

## Residual
#Residual of an approximate solution to a linear system is the difference
- $r=b-A\hat x$

Noticing that when both sides are multiplied by an arbitrary nonzero constant, the solution is unaffected but the residual is multiplied. Hence the size of the residual is meaningless.

#Relative_Residual: $\frac{||r||}{||A||\cdot||\hat x||}$
![[Pasted image 20240228010815.png]]
![[Pasted image 20240228011007.png]]
==Therefore, a large relative residual implies a large backward error in the matrix.==

---
# Solving Linear Systems
## Problem Transformations

#Premultiply: multiply from the left
- do it to both sides of the linear system by any non-singular matrix will not affect the solution
- that's how we simplify the problem

#Permutations: reorder the rows of a matrix/vector
![[Pasted image 20240228011649.png]]
- #Postmultiply a permutation matrix reorders the columns

#Diagnal_Scaling: pre-multiply a diagonal matrix scales the rows, post-multiply it scales the columns instead

## Triangular Linear Systems
#Lower_Triangular: all entries above the main diagonal are zero
#Upper_Triangular: all entries below the main diagonal are zero

For lower triangular system Lx=b, successive substitution is called #Forward_Substitution
- $x_1=b_1/l_{11}$
- $x_i=\left(b_i-\sum^{i-1}_{j=1}l_{ij}x_j\right)/l_{ii}$
	- i = 2,... , n
	- the result minus what we solved so far, then divided by the factor

For upper triangular system Ux=b, it is called #Backward_Error 

## Elementary Elimination Matrices
![[Pasted image 20240228152127.png]]
For where you don't want to keep, pick the corresponding column to eliminate the diagonal
- diagonal provides the original vector
- elimination column is a a column where entries are used to cancel the diagonal result
	- under the diagonal
![[Pasted image 20240228152654.png]]

## Gaussian Elimination and LU Factorization
#Gaussian_Elimination: Reduce the original form into an ==upper triangular form== by keep pre-multiplying elimination matrices
- also known as #LU_Factorization #LU_Decomposition because $M^{-1}=L$
![[Pasted image 20240228153139.png]]
![[Pasted image 20240228153333.png]]

$MA=U$ is upper triangular
$M^{-1}MA=A$
$M^{-1}U=A$
$M^{-1}=L$ is lower triangular
$LU=A$

$Ax=LUx=b$
$Ux=y$ is easy computed by back-substitution as long as we know the value of y
- $y=Mb$
- $M^{-1}y=b$
- $Ly=b$
$Ly=b$ is also easy computed by forward-substitution

## Pivoting
#Permutations to avoid potential problems

![[Pasted image 20240228155101.png]]
![[Pasted image 20240228155231.png]]
![[Pasted image 20240228155242.png]]
![[Pasted image 20240228163747.png]]
## Implementation of Gaussian Elimination
pass

## Complexity of Solving Linear Systems
pass

## Gauss-Jordan Elimination
#Gauss-Jordan_Elimination: eliminating all entries except one
![[Pasted image 20240228164118.png]]
![[Pasted image 20240228164202.png]]
Use this on augmented matrices are a good idea

## Solving Modified Problems
pass

## Improving Accuracy
pass

# Special Types of Linear Systems
#Symmetric: $A=A^T$
#Positive_Definite: $x^TAx>0$ for all $x\not=0$
#Banded: $\forall |i-j|>\beta,\,a_{ij}=0$
- beta is the #Bandwidth of A
- #Tridiagonal_Matrix: when beta=1
#Sparse: most entries are zero

## Symmetric Positive Definite Systems
pass

## Symmetric Indefinite Systems
pass

## Banded Systems
pass

## Iterative Methods for Linear Systems
pass

# Software for Linear Systems
pass
