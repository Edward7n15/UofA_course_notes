# Intro
#Numerical_Analysis == #Scientific_Computing

Most of the problems of #continuous mathematics CANNOT be solved exactly in a ==finite== number of steps and thus must be solved by a ==iterative== process that ultimately ==converges== to a solution.
- not iterate forever but until the answer is ==approximately== correct
- if the convergence is rapid, then even a problem that CAN be solved by finite algorithms may in some cases be better solved by iterative methods
	- systems of linear algebraic equations

## Problems
#Computational_Simulation is the representation and emulation of a physical system or process using a computer.
- simulate inaccessible situations
	- black hole
- save cost and time

#Computational_Simulation has become known as #Virtual_Prototyping.

Overall problem solving process:
1. develop a model
	1. #Mathematical_Modeling
2. develop algorithms
3. implement the algorithms in computer software
4. run the software to simulate the physical process numerically
5. represent the results in some comprehensible form such as graphical visualization
6. interpret and validate the results, repeating any or all steps if necessary

#Well_posed: an exist solution to a mathematical problem is unique and depends continuously on the problem data.
- small change in the data does not cause an abrupt, disproportionate change in the solution
	- such perturbations are usually inevitable

## Strategy
To replace a difficult problem with an easier one that has the same solution, or at least closely related solution.

- infinite-dimensional spaces to finite-dimensional spaces
- infinite processes to finite processes
- differential equations to algebraic equations
- nonlinear problems to linear problems
- high-order systems to low-order systems
- complicated functions to simple functions
- general matrices to matrices having a simpler form

---

# Approximations
## Sources

Beyond our control:
- modeling
	- some physical features of the problem or system under study may be simplified or omitted
- empirical measurements
	- lab instruments have finite precision
	- accuracy may be further limited by a small sample size, or readings obtained may be subject to random noise or systematic bias
- previous computations
	- input data may have been produced by a previous computational step whose results were only approximate

We do have some influence:
- truncation or discretization
	- some features of a model may be omitted or simplified
- rounding
	- the representation of real numbers is limited to some finite amount of precision and thus is generally inexact

#Error_Analysis: the study of effects of approximations on the accuracy and stability of numerical algorithms

## Absolute Error and Relative Error

#Absolute_Error= $\hat y - y$
- real distance

#Relative_Error=$\frac{\hat y-y}{y}$
- ratio of the distance and the true value
- ==notice== it is UNDERDEFINED if the true value is zero

#Significant_Digits: leading nonzero digit and all following digits
#Precision: the number of digits with which a number is expressed
#Accuracy: the number of ==correct== significant digits in approximating some desired quantity

## Data Error and Computational Error

Total error
$=\hat f(\hat x)-f(x)$
$=\hat f(\hat x)+\left(-f(\hat x)+f(\hat x)\right)-f(x)$
$=\left(\hat f(\hat x)-f(\hat x)\right)+\left(f(\hat x)-f(x)\right)$
= computational error + propagated data error

#Computational_Error: using the estimate input, the error between our estimate function and the true function

#Propagated_Data_Error: using the true function, the error between results using our estimate data and the true data

## Truncation Error and Rounding Error

#Computational_Error can be subdivided into truncation error and rounding error.

#Truncation_Error: due to truncating an infinite series, replacing derivatives by finite differences, or terminating an iterative sequence before convergence
- tends to dominate in problems involving integrals, derivatives, or nonlinearities

#Rounding_Error: due to finite precision, rounded arithmetic. The inexactness in the representation of real numbers and arithmetic operations upon them
- tends to dominate in purely algebraic problems with finite solution algorithms

By definition, #Computational_Error = #Truncation_Error + #Rounding_Error 

## Forward Error and Backward Error

#Forward_Error: $\Delta y=\hat y-y$

#Backward_Error:$\Delta x=\hat x-x$ where $f(\hat x)=\hat y=\hat f(x)$

![[Pasted image 20240227170913.png]]

## Sensitivity and Conditioning
#Insensitive== #Well_Conditioned: a given relative change in the input data causes a ==reasonably== commensurate relative change in the solution

#Sensitive == #Ill-Conditioned: the relative change in the solution can be much larger than that in the input data

#Condition_Number: ratio of the relative change in the solution to the relative change in the input
- CN >> 1: #Ill-Conditioned 
![[Pasted image 20240227171421.png]]
![[Pasted image 20240227171438.png]]

Usually we don't know the exact condition number but a estimate one. So, we set it as an upper bound which makes more sense
![[Pasted image 20240227172612.png]]

Another representation
![[Pasted image 20240227172813.png]]
The key is the Absolute Forward Error can be represented by derivative

#Absolute_Condition_Number: the ratio of the absolute change in the solution to the absolute change in the input, which is useful when the input or output is zero where we CANNOT use relative condition number
- root finding
	- find a special input where the output is 0
	- $\frac{\Delta y}{\Delta x}\approx |f'(x^*)|$

## Stability and Accuracy
#Stability in computational algorithm is analogous to #Conditioning of a mathematical problem
- #Stability refers to the effects of computational error
- #Conditioning refers to the effects of data error

#Accuracy refers to the ==closeness== of a computed solution to the true solution


---
# Computer Arithmetic
## Floating-Point Numbers
#Floating-point number system is the representation of real number system in a digital computer.
The basic idea is #Scientific_Notation
- $3.1415926\times 10^3$
- the decimal point moves as the power changes

![[Pasted image 20240227174258.png]]

## Normalization
#normalized: the leading digit $d_0$ is always nonzero unless the number represented is zero

## Properties of Floating-Point Systems
- finite
- discrete

$2(\beta-1)\beta^{p-1}(U-L+1)+1$
- 2 signs
- $\beta-1$ choices for the leading digits of the mantissa
- $\beta$ choices for the rest ${p-1}$ digits of the mantissa
- $U-L+1$ choices for the exponent
- 1 possibility for 0

#Underflow_Level = $\beta^L$
#Overflow_Level = $\beta^{U+1}(1-\beta^{-p})$
- $\beta^{U+1}-\beta^{U+1-p}$
- one more power is the ceiling
- precision is the smallest modifiable unit

## Rounding
#Rounding: the process of choosing a nearby floating-point number $fl(x)$ to approximate a given real number $x$
- the error is #Rounding_Error 

Common rounding rules
- #Chop
	- round towards zero
	- the base expansion of x is truncated after the p-1 digits
- #Round_to_Nearest
	- round to even
	- $fl(x)$ is the nearest floating-point number to x
		- in case of a tie, use the one whose last stored digit is even

## Machine Precision
#Unit_Roundoff= #Machine_Precision = #Machine_Epsilon
- $\epsilon_{mach}$
	- #Chop 
		- $\beta^{1-p}$
	- #Round_to_Nearest 
		- $\frac12 \beta^{1-p}$

Also the maximum possible #Relative_Error 
- $|\frac{fl(x)-x}{x}|\le\epsilon_{mach}$
- $fl(1+\epsilon)>1$

## Exceptional Values
- Inf
- NaN

## Floating-Point Arithmetic
$\sum^\infty_{n=1}\frac1n$
- terms smaller than the #Machine_Epsilon will be ignored
- thus the computation is finite

## Cancellation
#Rounding is not the only evil in finite-precision arithmetic. Subtraction between two p-digits numbers having the same sign and similar magnitudes yields a result with ==fewer== than p significant digits, and hence it is always exactly representable.

$\epsilon<\epsilon_{mach}\rightarrow(1+\epsilon)-(1-\epsilon)=1-1=0$
where we may have $2\epsilon>\epsilon_{mach}$

## Complex Arithmetic
pass


---
# Mathematical Software
pass

---
# Historical Notes and Further Reading
pass



