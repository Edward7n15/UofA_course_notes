Any point $x\in S$ (constraint set) are called feasible points.

- Hessian Matrix
	- $H_{i,j}=\frac{\partial^2f}{\partial x_i\cdot\partial x_j}$

Hessian Matrix consists of all ==second== partial derivatives of the same objective
- Jacobian Matrix consists of all partial derivatives of multiple objectives

## Second-Order Optimality Condition
- $H_f(x^*)$
	- positive
		- minimum
	- negative
		- maximum
	- indefinite
		- saddle point
	- singular
		- various pathological situations are possible

---

Unimodality: One unique extrema in the interval

Convex: Points between two endpoints are below the segment

---

# Newton's Method

$f(x+h)\approx f(x)+f'(x)h+\frac{f''(x)}{2}h^2$
- truncated Taylor series

Minimum when $h=-\frac{f'(x)}{f''(x)}$
- $x_{k+1}=x_k-\frac{f'(x_k)}{f''(x_k)}$
	- iteration scheme

---
# Steepest Descent Method
- use $\alpha$ instead of $\frac1{f''(x)}$ as the step size
- shrink the derivative to cancel out $x$

---

Strictly convex means there is a unique peak instead of a flat bottom

---

Condition number

---
# Level Sets

![[Pasted image 20240413163249.png]]

---
# Newton's

$x_{k+1}=x_k-\nabla f(x_k)\cdot H_f^{-1}$
- same idea as 2D

Inversion is expansive, solve $H_f(x_k)\cdot s_k=-\nabla f(x_k)$ instead
- $x_{k+1}=x_k+s_k$

---
# Steepest Descent
$x_{k+1}=x_k-\alpha_k\cdot\nabla f(x_k)$
- determine appropriate value for $a_k$ at each iteration
	- $\underset{a_k}{min}\quad f(x_k-\alpha_k\cdot\nabla f(x_k))$

